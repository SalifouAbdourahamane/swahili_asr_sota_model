{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U-AEjGHfrKv"
      },
      "outputs": [],
      "source": [
        "!pip install -qU bitsandbytes peft evaluate jiwer accelerate soundfile librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyZxozznfekU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset, Audio\n",
        "from collections import defaultdict\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"sartifyllc/Sartify_ITU_Zindi_Testdataset\", split=\"test\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio())\n",
        "\n",
        "# Initialize statistics collectors\n",
        "stats = {\n",
        "    'sample_rates': defaultdict(int),\n",
        "    'channels': defaultdict(int),\n",
        "    'durations': [],\n",
        "    'max_amplitudes': [],\n",
        "    'mean_amplitudes': []\n",
        "}\n",
        "\n",
        "# Process files to collect statistics\n",
        "print(\"\\nAnalyzing audio files...\")\n",
        "for item in tqdm(dataset, desc=\"Processing audio files\"):\n",
        "    audio = item['audio']\n",
        "\n",
        "    # Basic metadata\n",
        "    stats['sample_rates'][audio['sampling_rate']] += 1\n",
        "    channels = audio['array'].shape[0] if len(audio['array'].shape) > 1 else 1\n",
        "    stats['channels'][channels] += 1\n",
        "\n",
        "    # Duration calculation\n",
        "    duration = len(audio['array']) / audio['sampling_rate']\n",
        "    stats['durations'].append(duration)\n",
        "\n",
        "    # Amplitude analysis\n",
        "    waveform = torch.from_numpy(audio['array']).float()\n",
        "    stats['max_amplitudes'].append(torch.max(torch.abs(waveform)).item())\n",
        "    stats['mean_amplitudes'].append(torch.mean(torch.abs(waveform)).item())\n",
        "\n",
        "# Convert to pandas DataFrame for better analysis\n",
        "df_stats = pd.DataFrame({\n",
        "    'duration': stats['durations'],\n",
        "    'max_amplitude': stats['max_amplitudes'],\n",
        "    'mean_amplitude': stats['mean_amplitudes']\n",
        "})\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n=== Audio Dataset Statistics ===\")\n",
        "print(f\"\\nTotal files: {len(dataset)}\")\n",
        "print(f\"\\nSample Rates Distribution:\")\n",
        "for rate, count in sorted(stats['sample_rates'].items()):\n",
        "    print(f\"- {rate} Hz: {count} files ({count/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nChannels Distribution:\")\n",
        "for channels, count in sorted(stats['channels'].items()):\n",
        "    print(f\"- {'Mono' if channels == 1 else 'Stereo'}: {count} files ({count/len(dataset)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nDuration Statistics (seconds):\")\n",
        "print(df_stats['duration'].describe())\n",
        "\n",
        "print(\"\\nAmplitude Statistics:\")\n",
        "print(f\"- Max amplitude: mean={np.mean(stats['max_amplitudes']):.4f}, std={np.std(stats['max_amplitudes']):.4f}\")\n",
        "print(f\"- Mean amplitude: mean={np.mean(stats['mean_amplitudes']):.4f}, std={np.std(stats['mean_amplitudes']):.4f}\")\n",
        "\n",
        "# Plot distributions\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.hist(df_stats['duration'], bins=50)\n",
        "plt.title('Duration Distribution (seconds)')\n",
        "plt.xlabel('Duration')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(df_stats['max_amplitude'], bins=50)\n",
        "plt.title('Max Amplitude Distribution')\n",
        "plt.xlabel('Max Amplitude')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(df_stats['mean_amplitude'], bins=50)\n",
        "plt.title('Mean Amplitude Distribution')\n",
        "plt.xlabel('Mean Amplitude')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.scatter(df_stats['duration'], df_stats['max_amplitude'], alpha=0.3)\n",
        "plt.title('Duration vs Max Amplitude')\n",
        "plt.xlabel('Duration (s)')\n",
        "plt.ylabel('Max Amplitude')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print potential issues\n",
        "print(\"\\n=== Potential Audio Quality Issues ===\")\n",
        "quiet_threshold = 0.1\n",
        "short_threshold = 1.0  # seconds\n",
        "\n",
        "quiet_files = df_stats[df_stats['max_amplitude'] < quiet_threshold]\n",
        "short_files = df_stats[df_stats['duration'] < short_threshold]\n",
        "\n",
        "print(f\"\\nFiles with max amplitude < {quiet_threshold} (possibly too quiet): {len(quiet_files)}\")\n",
        "print(f\"Files with duration < {short_threshold} seconds: {len(short_files)}\")\n",
        "\n",
        "if len(quiet_files) > 0:\n",
        "    print(\"\\nSample quiet files statistics:\")\n",
        "    print(quiet_files.describe())\n",
        "\n",
        "if len(short_files) > 0:\n",
        "    print(\"\\nSample short files statistics:\")\n",
        "    print(short_files.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxVKfIN-fhTE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset with audio\n",
        "dataset = load_dataset(\"sartifyllc/Sartify_ITU_Zindi_Testdataset\", split=\"test\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio())\n",
        "\n",
        "def inspect_channels(sample_index=0, plot_waveform=True):\n",
        "    \"\"\"Inspect audio channels for a given sample\"\"\"\n",
        "    audio = dataset[sample_index]['audio']\n",
        "    waveform = audio['array']\n",
        "    sample_rate = audio['sampling_rate']\n",
        "    filename = dataset[sample_index]['filename']\n",
        "\n",
        "    # Channel info\n",
        "    num_channels = waveform.shape[0] if len(waveform.shape) > 1 else 1\n",
        "    duration = len(waveform[0]) / sample_rate if num_channels > 1 else len(waveform) / sample_rate\n",
        "\n",
        "    print(f\"\\nFile: {filename}\")\n",
        "    print(f\"Sample Rate: {sample_rate}Hz\")\n",
        "    print(f\"Channels: {'Stereo' if num_channels > 1 else 'Mono'}\")\n",
        "    print(f\"Duration: {duration:.2f} seconds\")\n",
        "    print(f\"Waveform shape: {waveform.shape}\")\n",
        "\n",
        "    if plot_waveform:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        if num_channels > 1:\n",
        "            plt.plot(waveform[0], label='Left Channel', alpha=0.7)\n",
        "            plt.plot(waveform[1], label='Right Channel', alpha=0.7)\n",
        "            plt.legend()\n",
        "        else:\n",
        "            plt.plot(waveform)\n",
        "\n",
        "        plt.title(f\"Audio Waveform\\n{filename}\")\n",
        "        plt.xlabel(\"Samples\")\n",
        "        plt.ylabel(\"Amplitude\")\n",
        "        plt.show()\n",
        "\n",
        "# Inspect first 5 files\n",
        "for i in range(5):\n",
        "    inspect_channels(i)\n",
        "\n",
        "# Get channel statistics for entire dataset\n",
        "print(\"\\nCalculating channel statistics for entire dataset...\")\n",
        "channel_counts = {'Mono': 0, 'Stereo': 0}\n",
        "\n",
        "for item in dataset:\n",
        "    waveform = item['audio']['array']\n",
        "    channels = waveform.shape[0] if len(waveform.shape) > 1 else 1\n",
        "    channel_counts['Mono' if channels == 1 else 'Stereo'] += 1\n",
        "\n",
        "print(\"\\nChannel Distribution:\")\n",
        "for channel_type, count in channel_counts.items():\n",
        "    print(f\"{channel_type}: {count} files ({count/len(dataset)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSpeFIA3f6yB"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Hugging Face login\n",
        "HF_TOKEN = \"\"\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bztTd-SagGEn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import datasets\n",
        "from datasets import load_dataset, Audio, Dataset\n",
        "import evaluate\n",
        "import librosa\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    TrainerCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================================\n",
        "# 1. CONFIGURATION\n",
        "# ==============================================\n",
        "# --- Paths and Datasets ---\n",
        "OUTPUT_DIR = \"whisper_turbo_finetuned\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "LOG_CSV = os.path.join(OUTPUT_DIR, \"training_log.csv\")\n",
        "EVAL_CSV = os.path.join(OUTPUT_DIR, \"eval_log.csv\")\n",
        "\n",
        "# New Model & Hub\n",
        "model_name_or_path = \"Abdoul27/whisper-turbo-v3-model\"\n",
        "task = \"transcribe\"\n",
        "\n",
        "# Datasets to load (Swahili-only)\n",
        "SWAHILI_DATASETS = [\n",
        "    {\"path\": \"Sunbird/salt\", \"config\": \"multispeaker-swa\", \"split\": \"train\"},\n",
        "    {\"path\": \"Sunbird/salt\", \"config\": \"studio-swa\", \"split\": \"train\"},\n",
        "]\n",
        "\n",
        "# Noise dataset for augmentation\n",
        "NOISE_DATASET_SPEC = {\"path\": \"Sunbird/urban-noise-uganda-61k\", \"config\": \"small\", \"split\": \"train\"}\n",
        "\n",
        "# --- LANGUAGE CONFIGURATION ---\n",
        "language = \"swahili\"\n",
        "language_abbr = \"sw\"\n",
        "TARGET_SAMPLING_RATE = 16000\n",
        "\n",
        "# --- Augmentation Hyperparameters ---\n",
        "MAX_REL_NOISE_AMP = 0.5\n",
        "P_NOISE_AUG = 0.5  # Probability of adding noise\n",
        "\n",
        "# ==============================================\n",
        "# 2. LOAD MODELS & PROCESSORS\n",
        "# ==============================================\n",
        "print(\"Loading model & processor:\", model_name_or_path)\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "\n",
        "# Prepare the model for 8-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define the LoRA configuration\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Wrap the base model with the PEFT model\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ==============================================\n",
        "# 3. LOAD & PREPARE DATASETS\n",
        "# ==============================================\n",
        "def safe_load_dataset(spec):\n",
        "    try:\n",
        "        if \"config\" in spec and spec[\"config\"]:\n",
        "            ds = load_dataset(spec[\"path\"], spec[\"config\"], split=spec.get(\"split\", \"train\"))\n",
        "        else:\n",
        "            ds = load_dataset(spec[\"path\"], split=spec.get(\"split\", \"train\"))\n",
        "        print(f\"[OK] Loaded dataset {spec['path']} config={spec.get('config')} split={spec.get('split')}\")\n",
        "        return ds\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load dataset {spec!r}: {e}\")\n",
        "        return None\n",
        "\n",
        "def normalize_dataset(ds, keep_text_name=\"text\"):\n",
        "    \"\"\"Normalize dataset features and column names.\"\"\"\n",
        "    if \"audio\" not in ds.column_names:\n",
        "        for col in ds.column_names:\n",
        "            if isinstance(ds.features.get(col), datasets.features.Audio):\n",
        "                ds = ds.rename_column(col, \"audio\")\n",
        "                break\n",
        "        else:\n",
        "            if \"source\" in ds.column_names:\n",
        "                ds = ds.rename_column(\"source\", \"audio\")\n",
        "\n",
        "    text_col = next((c for c in [\"transcription\", \"text\", \"sentence\", \"transcript\", \"target\"] if c in ds.column_names), None)\n",
        "    if text_col is None:\n",
        "        ds = ds.add_column(keep_text_name, [\"\"] * len(ds))\n",
        "    else:\n",
        "        if text_col != keep_text_name:\n",
        "            ds = ds.rename_column(text_col, keep_text_name)\n",
        "\n",
        "    keep_cols = [c for c in [\"audio\", keep_text_name] if c in ds.column_names]\n",
        "    ds = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
        "    ds = ds.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
        "    return ds\n",
        "\n",
        "print(\"\\nLoading and preparing datasets...\")\n",
        "train_parts = []\n",
        "val_parts = []\n",
        "for spec in SWAHILI_DATASETS:\n",
        "    ds = safe_load_dataset(spec)\n",
        "    if ds:\n",
        "        try:\n",
        "            ds = normalize_dataset(ds)\n",
        "            train_parts.append(ds)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to normalize dataset {spec}: {e}\")\n",
        "\n",
        "if not train_parts:\n",
        "    raise RuntimeError(\"No Swahili training datasets loaded.\")\n",
        "\n",
        "train_ds = datasets.concatenate_datasets(train_parts, axis=0)\n",
        "split = train_ds.train_test_split(test_size=0.02, seed=42)\n",
        "train_ds = split[\"train\"]\n",
        "val_ds = split[\"test\"]\n",
        "\n",
        "print(f\"Combined dataset: Train examples: {len(train_ds)}, Val examples: {len(val_ds)}\")\n",
        "\n",
        "# Load noise dataset\n",
        "noise_ds = safe_load_dataset(NOISE_DATASET_SPEC)\n",
        "noise_audio_arrays = []\n",
        "if noise_ds is not None:\n",
        "    try:\n",
        "        noise_ds = normalize_dataset(noise_ds, keep_text_name=\"text\")\n",
        "        noise_audio_arrays = [np.asarray(ex[\"audio\"][\"array\"], dtype=np.float32) for ex in noise_ds.select(range(min(len(noise_ds), 2000)))]\n",
        "        print(f\"Loaded {len(noise_audio_arrays)} noise clips for augmentation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed processing noise dataset: {e}\")\n",
        "        noise_audio_arrays = []\n",
        "\n",
        "# ==============================================\n",
        "# 4. DEFINE DATA COLLATOR AND METRICS\n",
        "# ==============================================\n",
        "def add_noise(x: np.ndarray, noise_pool: List[np.ndarray], max_rel_amp=MAX_REL_NOISE_AMP):\n",
        "    if not noise_pool:\n",
        "        return x\n",
        "    noise_clip = random.choice(noise_pool)\n",
        "    L = len(x)\n",
        "    if len(noise_clip) >= L:\n",
        "        start = random.randint(0, len(noise_clip) - L)\n",
        "        noise_seg = noise_clip[start:start + L]\n",
        "    else:\n",
        "        repeats = math.ceil(L / len(noise_clip))\n",
        "        noise_seg = np.tile(noise_clip, repeats)[:L]\n",
        "    audio_amp = np.max(np.abs(x)) + 1e-9\n",
        "    rel = random.random() * max_rel_amp\n",
        "    noise_scaled = noise_seg.astype(np.float32) * (rel * audio_amp)\n",
        "    y = x + noise_scaled\n",
        "    return np.clip(y, -1.0, 1.0).astype(np.float32)\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    noise_pool: List[np.ndarray] = None\n",
        "    add_noise_p: float = 0.5\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract audio arrays and texts\n",
        "        audio_arrays = [feature[\"audio\"][\"array\"] for feature in features]\n",
        "        texts = [feature.get(\"text\") or feature.get(\"transcription\") or feature.get(\"sentence\") or \"\" for feature in features]\n",
        "\n",
        "        # Apply noise augmentation directly on the fly\n",
        "        if self.noise_pool and random.random() < self.add_noise_p:\n",
        "             audio_arrays = [add_noise(arr, self.noise_pool) for arr in audio_arrays]\n",
        "\n",
        "        # Extract features and pad\n",
        "        input_features = self.processor.feature_extractor(audio_arrays, sampling_rate=TARGET_SAMPLING_RATE, return_tensors=\"pt\").input_features\n",
        "\n",
        "\n",
        "        # Tokenize and pad labels\n",
        "        tokenizer_output = self.processor.tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "        labels = tokenizer_output[\"input_ids\"].masked_fill(tokenizer_output[\"attention_mask\"].ne(1), -100)\n",
        "\n",
        "        # Prepare the final batch\n",
        "        batch = {\n",
        "            \"input_features\": input_features,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor, noise_pool=noise_audio_arrays, add_noise_p=P_NOISE_AUG)\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    pred_ids = eval_pred.predictions\n",
        "    label_ids = eval_pred.label_ids\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    pred_str = [p.lower().strip() for p in pred_str]\n",
        "    label_str = [l.lower().strip() for l in label_str]\n",
        "\n",
        "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer, \"cer\": cer}\n",
        "\n",
        "# ==============================================\n",
        "# 5. DEFINE TRAINING ARGUMENTS\n",
        "# ==============================================\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    num_train_epochs=3,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=400,\n",
        "    eval_steps=200,\n",
        "    logging_steps=200,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# ==============================================\n",
        "# 6. INITIALIZE AND START TRAINING\n",
        "# ==============================================\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.tokenizer,\n",
        "\n",
        ")\n",
        "\n",
        "processor.save_pretrained(training_args.output_dir)\n",
        "\n",
        "print(\"\\nStarting PEFT training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "trainer.save_model(training_args.output_dir)\n",
        "print(\"Artifacts saved to:\", training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGfP3QiigcH_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import pipeline, WhisperProcessor\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress all warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Use our newly finetuned model\n",
        "MODEL_ID = \"./whisper_turbo_finetuned\"\n",
        "OUTPUT_FILE = \"submission_turbo_finetuned_beam_search.csv\"\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "print(\"Loading the newly finetuned Swahili ASR model...\")\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load processor and get forced_decoder_ids for the older approach\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"swahili\", task=\"transcribe\")\n",
        "\n",
        "# --- 2. Pipeline Creation ---\n",
        "# The pipeline automatically handles the model and processor loading\n",
        "print(\"Setting up ASR pipeline with beam search...\")\n",
        "asr_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=MODEL_ID,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        "    model_kwargs={\n",
        "        \"attn_implementation\": \"flash_attention_2\"\n",
        "    } if is_flash_attn_2_available() else {}, # Removed SDPA as it's the default\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"sartifyllc/Sartify_ITU_Zindi_Testdataset\", split=\"test\")\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n",
        "\n",
        "print(f\"Total files to process: {len(dataset)}\")\n",
        "print(f\"\\nStarting transcription with {MODEL_ID} model... 🎤\")\n",
        "\n",
        "results = []\n",
        "for i, row in enumerate(tqdm(dataset, desc=f\"Transcribing with {MODEL_ID}\")):\n",
        "    try:\n",
        "        # Get audio array\n",
        "        audio_array = row[\"audio\"][\"array\"]\n",
        "\n",
        "        # To use beam search, we pass the appropriate arguments to generate_kwargs.\n",
        "        transcription_result = asr_pipeline(\n",
        "            audio_array,\n",
        "            generate_kwargs={\n",
        "                'forced_decoder_ids': forced_decoder_ids,\n",
        "                'num_beams': 3,\n",
        "                'max_new_tokens': 440,\n",
        "                'early_stopping': True,\n",
        "                'repetition_penalty': 1.2\n",
        "            }\n",
        "        )\n",
        "        transcription = transcription_result[\"text\"].strip().lower()\n",
        "\n",
        "        results.append({\n",
        "            'filename': row['filename'],\n",
        "            'text': transcription if transcription else \"\" # Use empty string for no transcription\n",
        "        })\n",
        "\n",
        "        # Print details for first 5 files\n",
        "        if i < 5:\n",
        "            duration = len(audio_array) / SAMPLE_RATE\n",
        "            print(f\"\\n[{i+1}/5] File: {row['filename']}\")\n",
        "            print(f\"Audio duration: {duration:.2f}s\")\n",
        "            print(f\"Transcription: '{transcription}'\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing {row['filename']}: {str(e)}\")\n",
        "        results.append({\n",
        "            'filename': row['filename'],\n",
        "            'text': \"\"  # Default fallback\n",
        "        })\n",
        "\n",
        "print(f\"\\nProcessed {len(results)} files successfully\")\n",
        "print(\"Saving results to CSV...\")\n",
        "\n",
        "# Create submission dataframe\n",
        "submission_df = pd.DataFrame(results)\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"\\nSubmission file saved to {OUTPUT_FILE}\")\n",
        "\n",
        "# Show sample results\n",
        "print(\"\\nSample transcriptions:\")\n",
        "print(submission_df.head(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
